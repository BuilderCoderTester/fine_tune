{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13740276,"sourceType":"datasetVersion","datasetId":8742549}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install protobuf==3.20.3 --force-reinstall\n# !pip install trl\n# !pip install bitsandbytes\n# # print(\"completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T07:51:01.029348Z","iopub.execute_input":"2025-11-15T07:51:01.029608Z","iopub.status.idle":"2025-11-15T07:51:01.032988Z","shell.execute_reply.started":"2025-11-15T07:51:01.029571Z","shell.execute_reply":"2025-11-15T07:51:01.032242Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport os \nfrom datasets import load_dataset\nfrom transformers import (\nAutoModelForCausalLM,\nAutoTokenizer,\nBitsAndBytesConfig,\nHfArgumentParser,\nTrainingArguments,\nPipeline,\nlogging\n)\n\nfrom peft import LoraConfig ,PeftModel\nfrom trl import SFTTrainer\n\nimport re\ndataset_1 = load_dataset(\"timdettmers/openassistant-guanaco\")\ndataset_1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T07:51:02.481460Z","iopub.execute_input":"2025-11-15T07:51:02.482052Z","iopub.status.idle":"2025-11-15T07:51:03.897100Z","shell.execute_reply.started":"2025-11-15T07:51:02.482026Z","shell.execute_reply":"2025-11-15T07:51:03.896514Z"}},"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\nWARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 9846\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 518\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"dataset_1['train'][2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T08:00:29.806864Z","iopub.execute_input":"2025-11-15T08:00:29.807398Z","iopub.status.idle":"2025-11-15T08:00:29.812302Z","shell.execute_reply.started":"2025-11-15T08:00:29.807375Z","shell.execute_reply":"2025-11-15T08:00:29.811722Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'text': '### Human: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?### Assistant: Sure! Let\\'s say you want to build a model which can distinguish between images of cats and dogs. You gather your dataset, consisting of many cat and dog pictures. Then you put them through a neural net of your choice, which produces some representation for each image, a sequence of numbers like [0.123, 0.045, 0.334, ...]. The problem is, if your model is unfamiliar with cat and dog images, these representations will be quite random. At one time a cat and a dog picture could have very similar representations (their numbers would be close to each other), while at others two cat images may be represented far apart. In simple terms, the model wouldn\\'t be able to tell cats and dogs apart. This is where contrastive learning comes in.\\n\\nThe point of contrastive learning is to take pairs of samples (in this case images of cats and dogs), then train the model to \"pull\" representations of similar pairs (cat-cat or dog-dog) closer to each other and \"push\" representations of different pairs (cat-dog) apart. After doing this for a sufficient number of steps, your model will be able to produce unique, reliable representations for cats and dogs, in essence tell them apart.\\n\\nThis method is not limited to images, you can typically use it with any dataset that has similar and dissimilar data points.### Human: I didn\\'t understand how pulling and pushing works. What are we pulling or pushing? And how does it work in terms of code or maths?### Assistant: It has to do with a process called differentiation, which means finding a derivative: a function that tells us whether another function is going up or down at a given point. For example, the derivative of `f(x) = x` is `f\\'(x) = 1`, because it\\'s always going up at a 1:1 ratio. This can be done for a variety of functions; notably, if you know the derivatives of `f(x)` and `g(x)`, you can also get the derivative of `f(g(x))` using a formula called the chain rule. Neural networks happen to be made of differentiable functions, so we can take the derivative of parts or all of it.\\n\\nTo use this for \"pushing\" and \"pulling\", we\\'ll put two images through the neural network. Let\\'s say the images are of a cat and a dog, so we want to increase the distance between the two. We pick one neuron weight from the network and make it a variable `x`, then construct a function that calculates the output of the network based on it and all the other parameters; let\\'s call it `N(x)`. The distance between the cat and dog outputs would be `f(x) = N(cat) - N(dog)`. (Of course, the real output would have more than 1 dimension, but we\\'re simplifying.) We now want to nudge the weight such that it moves the two outputs slightly further apart. For that, we can simply take the derivative! If `f\\'(x)` is positive, that means that increasing the weight will move them further apart, so we should do that. If it\\'s negative, then it\\'ll move them closer, so we\\'ll want to slightly decrease the weight instead. Apply this to all the neurons enough times and your network will soon converge to a pretty good cat-dog separator!'}"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available:\", torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, using CPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:31:37.277707Z","iopub.execute_input":"2025-11-14T17:31:37.278329Z","iopub.status.idle":"2025-11-14T17:31:37.282829Z","shell.execute_reply.started":"2025-11-14T17:31:37.278304Z","shell.execute_reply":"2025-11-14T17:31:37.282207Z"}},"outputs":[{"name":"stdout","text":"GPU is available: Tesla T4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# bnb configuration","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                     # Enable 4-bit weights\n    bnb_4bit_quant_type=\"nf4\",             # Normalized Float 4 (best for LLMs)\n    bnb_4bit_compute_dtype=torch.float16,  # Compute in FP16 for speed & tensor cores\n    bnb_4bit_use_double_quant=True         # Nested quantization to improve accuracy\n)\nprint(\"Complterd\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:31:40.865792Z","iopub.execute_input":"2025-11-14T17:31:40.866393Z","iopub.status.idle":"2025-11-14T17:31:40.872460Z","shell.execute_reply.started":"2025-11-14T17:31:40.866367Z","shell.execute_reply":"2025-11-14T17:31:40.871527Z"}},"outputs":[{"name":"stdout","text":"Complterd\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"model_name = \"NousResearch/Llama-2-7b-chat-hf\"\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\nnew_model = \"Llama-2-7b-chat-finetune\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:31:44.213037Z","iopub.execute_input":"2025-11-14T17:31:44.213787Z","iopub.status.idle":"2025-11-14T17:31:44.217390Z","shell.execute_reply.started":"2025-11-14T17:31:44.213759Z","shell.execute_reply":"2025-11-14T17:31:44.216559Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:31:50.564689Z","iopub.execute_input":"2025-11-14T17:31:50.565473Z","iopub.status.idle":"2025-11-14T17:33:06.334605Z","shell.execute_reply.started":"2025-11-14T17:31:50.565439Z","shell.execute_reply":"2025-11-14T17:33:06.333523Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d158760f78a8488ca89d1b8cf8a5e010"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35cd6774e6be41b7a2fc511d0f2ca42a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7916741ec207424b92e3e7033e48ce84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6364c6c287f24c5f96e3e4fddc4a3690"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f099bcacd5894ee18dee57fe4a57cad1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d8e7377d8d45ae996825bf9ec48f44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8573029a174eaba42cb4130ef2d04e"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Check the device of the first parameter\nfirst_param = next(model.parameters())\nprint(\"First parameter device:\", first_param.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:33:06.336087Z","iopub.execute_input":"2025-11-14T17:33:06.336409Z","iopub.status.idle":"2025-11-14T17:33:06.342604Z","shell.execute_reply.started":"2025-11-14T17:33:06.336377Z","shell.execute_reply":"2025-11-14T17:33:06.341727Z"}},"outputs":[{"name":"stdout","text":"First parameter device: cuda:0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:37:11.827428Z","iopub.execute_input":"2025-11-14T17:37:11.828318Z","iopub.status.idle":"2025-11-14T17:37:11.832441Z","shell.execute_reply.started":"2025-11-14T17:37:11.828283Z","shell.execute_reply":"2025-11-14T17:37:11.831523Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Llama tokenizer and Lora","metadata":{}},{"cell_type":"code","source":"# load Llama tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# load LoRA configuration\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:39:07.583386Z","iopub.execute_input":"2025-11-14T17:39:07.584337Z","iopub.status.idle":"2025-11-14T17:39:09.241775Z","shell.execute_reply.started":"2025-11-14T17:39:07.584302Z","shell.execute_reply":"2025-11-14T17:39:09.240909Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d3c0cb25dc340c4a9f33dde51457c82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce5a693adcb640f1a2bcff8e31e6ef9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40bb655c5b374ffb9b6ad9595f21295d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e24961d3085e4b5795cef34c7e6194c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cb2cef9b57d49089948bebc110e1d18"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def transform_conversion(example):\n    conversation_text = example['text']\n    segments = conversation_text.split(\"###\")\n\n    reformat_segments = []\n\n    # Iterate through segments in pairs (Human / Assistant)\n    for i in range(1, len(segments) - 1, 2):\n        human_text = segments[i].strip().replace('Human:', '').strip()\n\n        if i + 1 < len(segments):\n            assistant_text = segments[i + 1].strip().replace('Assistant:', '').strip()\n            reformat_segments.append(f\"<s>[INST] {human_text} [/INST] {assistant_text} </s>\")\n        else:\n            reformat_segments.append(f\"<s>[INST] {human_text} [/INST]</s>\")\n\n    return {'text': ''.join(reformat_segments)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T14:02:17.199373Z","iopub.execute_input":"2025-11-14T14:02:17.199639Z","iopub.status.idle":"2025-11-14T14:02:17.205689Z","shell.execute_reply.started":"2025-11-14T14:02:17.199615Z","shell.execute_reply":"2025-11-14T14:02:17.205149Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# QLoRA Parameters \nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\n\n# Quantization\nuse_4bit = True\nbnb_compute_dtype = \"float16\"          # ✅ corrected name\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False               # ✅ corrected spelling\n\n# Training configuration\noutput_dir = \"./qlora-output\"          # ✅ should not be empty\nnum_train_epochs = 1                   # ✅ corrected plural (used by HF Trainer)\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 1\nper_device_eval_batch_size = 1\n\ngradient_accumulation_steps = 2        # ✅ corrected spelling\ngradient_checkpointing = True          # ✅ corrected spelling\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\n\nlr_scheduler_type = \"cosine\"\nmax_steps = -1 \nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 0 \nlogging_steps = 25\n\n# SFT parameters \nmax_seq_length = 512\npacking = False\ndevice_map = {\"\": 0}                   # ✅ keep empty key if using single GPU\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:12:20.841047Z","iopub.execute_input":"2025-11-14T17:12:20.841312Z","iopub.status.idle":"2025-11-14T17:12:20.852324Z","shell.execute_reply.started":"2025-11-14T17:12:20.841296Z","shell.execute_reply":"2025-11-14T17:12:20.851569Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# TrainingArguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./qlora-output\",                 # Where checkpoints and logs are saved\n    num_train_epochs=1,     # Total epochs\n    per_device_train_batch_size=1,         # ✅ T4 16GB + 4-bit → usually batch_size=1 or 2\n    per_device_eval_batch_size=1,          # Evaluation batch size\n    gradient_accumulation_steps=4,         # Accumulate grads to simulate larger batch\n    optim=\"adamw_torch\",                   # Optimizer, works well with small GPUs\n    save_steps=200,                        # Save checkpoint every N steps\n    logging_steps=50,                      \n\n    learning_rate=2e-4,                     # Adjust for LoRA fine-tuning\n    weight_decay=0.0,                      \n\n    fp16=True,                              # Mixed precision for Tesla T4\n    max_grad_norm=1.0,                      \n    max_steps=-1,                           # Use max_steps=-1 if you want full epochs\n    warmup_ratio=0.03,                      \n\n    group_by_length=True,                   # Group sequences by length → saves memory\n    lr_scheduler_type=\"cosine\",             # Cosine scheduler works well with LoRA\n\n    report_to=\"tensorboard\",                # TensorBoard logging\n    ddp_find_unused_parameters=False        # Not using multi-GPU, safe for single GPU\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:42:06.041654Z","iopub.execute_input":"2025-11-14T17:42:06.041938Z","iopub.status.idle":"2025-11-14T17:42:06.077145Z","shell.execute_reply.started":"2025-11-14T17:42:06.041916Z","shell.execute_reply":"2025-11-14T17:42:06.076403Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from peft import PeftModel, get_peft_model\nfrom transformers import default_data_collator\nfrom trl import SFTTrainer\n\n# 1️⃣ Attach LoRA (only once)\nif not isinstance(model, PeftModel):\n    model = get_peft_model(model, peft_config)\n    print(\"LoRA adapter applied.\")\nelse:\n    print(\"Model already has LoRA applied.\")\n\n# 2️⃣ Ensure training args are set for 4-bit\ntraining_arguments.fp16 = True   # required for 4-bit\ntraining_arguments.bf16 = False\n\n# 3️⃣ Attach tokenizer to model so TRL can use it\nmodel.tokenizer = tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:47:54.458686Z","iopub.execute_input":"2025-11-14T17:47:54.459248Z","iopub.status.idle":"2025-11-14T17:47:54.464306Z","shell.execute_reply.started":"2025-11-14T17:47:54.459225Z","shell.execute_reply":"2025-11-14T17:47:54.463476Z"}},"outputs":[{"name":"stdout","text":"Model already has LoRA applied.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:47:57.082594Z","iopub.execute_input":"2025-11-14T17:47:57.082932Z","iopub.status.idle":"2025-11-14T17:47:57.094678Z","shell.execute_reply.started":"2025-11-14T17:47:57.082902Z","shell.execute_reply":"2025-11-14T17:47:57.093863Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\nimport torch\n\ndef data_collator_with_labels(batch):\n    # batch is a list of dicts like {'input_ids': [...]}\n    input_ids = [torch.tensor(example['input_ids']) for example in batch]\n    \n    # pad to max length in this batch\n    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n    \n    # labels are the same as input_ids for causal LM\n    labels = input_ids.clone()\n    \n    return {'input_ids': input_ids, 'labels': labels, 'attention_mask': (input_ids != 0).long()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:48:03.090244Z","iopub.execute_input":"2025-11-14T17:48:03.090960Z","iopub.status.idle":"2025-11-14T17:48:03.095349Z","shell.execute_reply.started":"2025-11-14T17:48:03.090931Z","shell.execute_reply":"2025-11-14T17:48:03.094588Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"new_dataset = dataset_1\nnew_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:48:04.465578Z","iopub.execute_input":"2025-11-14T17:48:04.466256Z","iopub.status.idle":"2025-11-14T17:48:04.470781Z","shell.execute_reply.started":"2025-11-14T17:48:04.466231Z","shell.execute_reply":"2025-11-14T17:48:04.469986Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 9846\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 518\n    })\n})"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"# SFT trainer","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=new_dataset['train'],\n    args=training_arguments,\n    data_collator=data_collator_with_labels,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:48:08.623570Z","iopub.execute_input":"2025-11-14T17:48:08.623857Z","iopub.status.idle":"2025-11-14T17:48:09.541197Z","shell.execute_reply.started":"2025-11-14T17:48:08.623835Z","shell.execute_reply":"2025-11-14T17:48:09.540600Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import torch\nprint(torch.cuda.get_device_name(0))\nprint(torch.version.cuda)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:48:12.066810Z","iopub.execute_input":"2025-11-14T17:48:12.067660Z","iopub.status.idle":"2025-11-14T17:48:12.072284Z","shell.execute_reply.started":"2025-11-14T17:48:12.067626Z","shell.execute_reply":"2025-11-14T17:48:12.071319Z"}},"outputs":[{"name":"stdout","text":"Tesla T4\n12.4\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# trainig is done ","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:48:16.474442Z","iopub.execute_input":"2025-11-14T17:48:16.475253Z","iopub.status.idle":"2025-11-14T20:52:54.517999Z","shell.execute_reply.started":"2025-11-14T17:48:16.475216Z","shell.execute_reply":"2025-11-14T20:52:54.517439Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2462' max='2462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2462/2462 3:04:25, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.683700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.329800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.323900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.277000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.298300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.378400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.283500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.286700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.353200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.302800</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.321000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.295500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.305000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.337800</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.343500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.331800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.309100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.268400</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.297100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.296200</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>1.258500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.278600</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>1.311200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.356700</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.318500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.322800</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>1.340600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.255500</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>1.302900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.363500</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>1.299600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.296200</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>1.328300</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.264700</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>1.228800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.284700</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>1.250800</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.238800</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>1.333100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.250200</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>1.280800</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.362600</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>1.338100</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.284900</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>1.325300</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>1.341100</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>1.233800</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.292500</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>1.283400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2462, training_loss=1.31182559034284, metrics={'train_runtime': 11075.7092, 'train_samples_per_second': 0.889, 'train_steps_per_second': 0.222, 'total_flos': 1.6359454553874432e+17, 'train_loss': 1.31182559034284, 'entropy': 1.4191851758438607, 'num_tokens': 4123966.0, 'mean_token_accuracy': 0.6710686845623929, 'epoch': 1.0})"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/qlora_model\")\ntokenizer.save_pretrained(\"/kaggle/working/qlora_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:53:10.211615Z","iopub.execute_input":"2025-11-14T20:53:10.211897Z","iopub.status.idle":"2025-11-14T20:53:10.522105Z","shell.execute_reply.started":"2025-11-14T20:53:10.211876Z","shell.execute_reply":"2025-11-14T20:53:10.521466Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/qlora_model/tokenizer_config.json',\n '/kaggle/working/qlora_model/special_tokens_map.json',\n '/kaggle/working/qlora_model/tokenizer.model',\n '/kaggle/working/qlora_model/added_tokens.json',\n '/kaggle/working/qlora_model/tokenizer.json')"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"!zip -r /kaggle/working/qlora_model.zip /kaggle/working/qlora_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:54:27.431974Z","iopub.execute_input":"2025-11-14T20:54:27.432274Z","iopub.status.idle":"2025-11-14T20:54:28.661467Z","shell.execute_reply.started":"2025-11-14T20:54:27.432252Z","shell.execute_reply":"2025-11-14T20:54:28.660556Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/qlora_model/ (stored 0%)\n  adding: kaggle/working/qlora_model/added_tokens.json (stored 0%)\n  adding: kaggle/working/qlora_model/adapter_config.json (deflated 55%)\n  adding: kaggle/working/qlora_model/tokenizer.json (deflated 85%)\n  adding: kaggle/working/qlora_model/README.md (deflated 65%)\n  adding: kaggle/working/qlora_model/tokenizer.model","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 55%)\n  adding: kaggle/working/qlora_model/tokenizer_config.json (deflated 72%)\n  adding: kaggle/working/qlora_model/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/qlora_model/special_tokens_map.json (deflated 72%)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# load the model and weights  and check the inference rate ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer , AutoModelForCausalLM\nfrom peft import PeftModel\nbase_model = \"NousResearch/Llama-2-7b-chat-hf\"\nlora_path = \"/kaggle/input/tuned-model/kaggle/working/qlora_model\"\n\n\ntokenizer = AutoTokenizer.from_pretrained(lora_path)\n\nmodel= AutoModelForCausalLM.from_pretrained(\n    base_model ,\n    torch_dtype = \"auto\",\n    device_map= \"auto\"\n)\n\nmodel = PeftModel.from_pretrained(model, lora_path)\nmodel.eval()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T07:51:24.836094Z","iopub.execute_input":"2025-11-15T07:51:24.836763Z","iopub.status.idle":"2025-11-15T07:52:25.614308Z","shell.execute_reply.started":"2025-11-15T07:51:24.836737Z","shell.execute_reply":"2025-11-15T07:52:25.613473Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e20c88f3abd344d2864a1a28b848957a"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"881fce077bb04ec3b4947da5bf687911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd23d449729e4455bd982c922e45cab3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f7b16111b1d4dada4e923ddba8e32cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d47d284639764feb874e144704bd6286"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e447b11790a34d81a63f2340f80e3956"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1004b091b55f44b7aa98f2a2d44b5865"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# new Prompting ","metadata":{}},{"cell_type":"code","source":"prompt = \"Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\nwith torch.no_grad():\n    output = model.generate(\n        **inputs,\n        max_new_tokens=200,\n        temperature=0.7\n    )\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T08:00:53.084911Z","iopub.execute_input":"2025-11-15T08:00:53.085608Z","iopub.status.idle":"2025-11-15T08:01:05.685753Z","shell.execute_reply.started":"2025-11-15T08:00:53.085569Z","shell.execute_reply":"2025-11-15T08:01:05.684892Z"}},"outputs":[{"name":"stdout","text":"Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\n1. What is contrastive learning?\n2. How does contrastive learning work?\n3. What are the benefits of contrastive learning?\n4. What are the challenges of contrastive learning?\n5. How does contrastive learning compare to other machine learning methods?\n6. What are some examples of contrastive learning in action?\n7. How can I get started with contrastive learning?\n\nContrastive learning is a type of machine learning that involves training a model to distinguish between two different types of data. The model is trained to recognize the differences between the two types of data, and to use those differences to make predictions or classify new data.\n\nIn contrastive learning, the model is trained on a dataset that contains both positive and negative examples. The positive examples are similar to each other, while the negative examples are dissimilar. For example, if the model is trained on images of cats and dogs, the positive examples\n","output_type":"stream"}],"execution_count":16}]}